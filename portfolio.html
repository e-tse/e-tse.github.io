<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Projects - Ethan Tse</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;500;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="styles.css">
</head>

<body>
    <header>
        <nav>
            <a href="index.html">Home</a>
            <a href="portfolio.html">Portfolio</a>
            <a href="personal.html">Personal</a>
            <a href="contact.html">Contact</a>
        </nav>
    </header>

    <main>
        <h1>Portfolio</h1>
        <p class="spaced">Here is a cummulative summary of all the major groups and projects I've worked on since the
            begining of undergrad.
            This list is in reverse chronological order, and categorized by organization. You can also
            check out my recent resume <a target="_blank" rel="noopener noreferrer" href="media/Ethan Resume.pdf">
                here</a>.
        </p>

        <!-- DART -->
        <details>
            <summary>Georgia Tech - DART Lab</summary>
            <p><strong>Title:</strong> Graduate Research Assistant AND Graduate Teaching Assistant</p>
            <p><strong>Dates:</strong> Aug 2024 – Present</p>

            <h3>Multimodal Robotics</h3>
            <figure>
                <video controls style="max-width: 100%; border-radius: 8px;" class="media">
                    <source src="media/gatech/multimodal.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <figcaption>Final demonstration of autonomous landing on a moving ground vehicle. The multi-rotor
                    is able to determine it's location relative to the ground vehicle using infrared beacons. Video
                    credit Ethan Tse and DART Lab.
                </figcaption>
            </figure>
            <p>Designed a new unmanned ground vehicle that performs multimodal (wheeled and gliding) locomotion to
                surmount obstacles.</p>


            <h3>Ground Robot and Arm Setup</h3>
            <figure>
                <video controls style="max-width: 100%; border-radius: 8px;" class="media">
                    <source src="media/gatech/arm_moving1.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <figcaption>Final demonstration of autonomous landing on a moving ground vehicle. The multi-rotor
                    is able to determine it's location relative to the ground vehicle using infrared beacons. Video
                    credit Ethan Tse and DART Lab.
                </figcaption>
            </figure>
            <p>Designed a new unmanned ground vehicle that performs multimodal (wheeled and gliding) locomotion to
                surmount obstacles.</p>


            <h3>ME 4451 - Head Teaching Assistant</h3>


        </details>


        <!-- NASA -->
        <details>
            <summary>NASA</summary>
            <p><strong>Title:</strong> Intern</p>
            <p><strong>Dates:</strong> May 2023 – July 2022</p>

            <h3>Lunar Surface Mobility Demo</h3>
            <p class="spaced">At NASA, I worked on project focused on multiagent lunar surface mapping, navigation, and
                hazard avoidance. The idea behind the project is, prior to sending astronauts back to the moon, we could
                send a team of
                robots that could map out a potential landing site and perform basic construction and landing zone
                setup.
                Below I've included a system overview, and then a few of the key contributions I personally made to the
                project.
            </p>
            <p>
                I collaborated on this project with my fellow intern Mukund Krishnakumar.
                You can download our end of summer presentation
                <a href="media/NASA/Summer 2023 Presentation final.pptx.pdf" target="_blank"
                    rel="noopener noreferrer">here</a>.
            </p>


            <h4>System Overview</h4>
            <div class="media-row">
                <figure>
                    <img src="media/NASA/rovers.jpg" alt="rovers">
                    <figcaption>Two Rover Robotics Rovers. Photo credit Ethan Tse and NASA.</figcaption>
                </figure>
                <figure>
                    <img src="media/NASA/LRT.png" alt="LRT">
                    <figcaption>Lunar Regolith Terrain. Photo credit Mike Zanetti and NASA</figcaption>
                </figure>
            </div>

            <p>This project involved two Rover Robotics, 4WD Rover Pro’s. One of these robots was equipped with an
                Ouster OS-1 LiDAR
                (Light Detection and Ranging) sensor, and the other with a Smart Video Guidance Sensor (SVGS) that can
                determine its local
                position and orientation (pose) relative to an LED target. We designed this system to work on the Lunar
                Regolith Terrain,
                which is a 125 foot by 125 foot field of lunar regolith (moon soil) simulant. The mission plan is to
                have the LiDAR rover
                explore an unknown landing sight, create a hazard map of obstacles and untraversable terrain, then have
                the SVGS rover perform
                basic construction tasks using this map.
            </p>

            <figure>
                <img src="media/NASA/ROS Overview.PNG" alt="ROS 2 overview" class="media" style="border: 1px solid #000000;">
                <figcaption>Lunar Regolith Terrain. Photo credit Mike Zanetti and NASA</figcaption>
            </figure>



            <h4>LiDAR SLAM Fix</h4>
            <div class="media-row">
                <figure>
                    <img src="media/NASA/broken1.png" alt="broken lidar 1">
                    <figcaption>Two Rover Robotics Rovers. Photo credit Ethan Tse and NASA.</figcaption>
                </figure>
                <figure>
                    <img src="media/NASA/broken2.jpg" alt="broken lidar 2">
                    <figcaption>Lunar Regolith Terrain. Photo credit Mike Zanetti and NASA</figcaption>
                </figure>
            </div>

            <figure>
                <img src="media/NASA/clean1.png" alt="ROS 2 overview" class="media">
                <figcaption>Lunar Regolith Terrain. Photo credit Mike Zanetti and NASA</figcaption>
            </figure>

            <h4>Costmap Optimization</h4>
            <div class="media-row">
                <figure>
                    <img src="media/NASA/clean2.png" alt="broken lidar 1">
                    <figcaption>Two Rover Robotics Rovers. Photo credit Ethan Tse and NASA.</figcaption>
                </figure>
                <figure>
                    <img src="media/NASA/unit normals.png" alt="broken lidar 2">
                    <figcaption>Lunar Regolith Terrain. Photo credit Mike Zanetti and NASA</figcaption>
                </figure>
            </div>

            <div class="media-row">
                <figure>
                    <img src="media/NASA/raw costmap.png" alt="broken lidar 1">
                    <figcaption>Two Rover Robotics Rovers. Photo credit Ethan Tse and NASA.</figcaption>
                </figure>
                <figure>
                    <img src="media/NASA/cleaned costmap.png" alt="broken lidar 2" style="border: 1px solid #000000;">
                    <figcaption>Lunar Regolith Terrain. Photo credit Mike Zanetti and NASA</figcaption>
                </figure>
            </div>



            <h4>Simple Go To Pose Controller</h4>

            <figure>
                <video controls style="max-width: 100%; border-radius: 8px;" class="media">
                    <source src="media/NASA/go_to_pose.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <figcaption>Final demonstration of autonomous landing on a moving ground vehicle. The multi-rotor
                    is able to determine it's location relative to the ground vehicle using infrared beacons. Video
                    credit Adam Garlow and AREAL.
                </figcaption>
            </figure>


            <figure>
                <video controls style="max-width: 100%; border-radius: 8px;" class="media">
                    <source src="media/NASA/follow_the_leader.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <figcaption>Final demonstration of autonomous landing on a moving ground vehicle. The multi-rotor
                    is able to determine it's location relative to the ground vehicle using infrared beacons. Video
                    credit Adam Garlow and AREAL.
                </figcaption>
            </figure>

        </details>

        <!-- AREAL -->
        <details>
            <summary>Georgia Tech - AREAL lab</summary>
            <p><strong>Title:</strong> Undergraduate Research Assistant</p>
            <p><strong>Dates:</strong> August 2021 – May 2024</p>

            <h3>Autonomous Quadrotor Landing on Moving Vehicle</h3>
            <p class="spaced">The main project I worked on at AREAL focused on autonomously landing a multi-rotor
                vehicle on a moving ground vehicle.
                As an undergraduate researcher, I was tasked with developing software and mechatronic
                features that enable autonomous capabilities for multi-rotor vehicles. </p>

            <figure>
                <video controls style="max-width: 100%; border-radius: 8px;" class="media">
                    <source src="media/gatech/landing_1.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <figcaption>Final demonstration of autonomous landing on a moving ground vehicle. The multi-rotor
                    is able to determine it's location relative to the ground vehicle using infrared beacons. Video
                    credit Adam Garlow and AREAL.
                </figcaption>
            </figure>

            <p class="spaced">My most significant contribution to this project was an infrared (IR) beacon localization
                package,
                which implements a coded beacon system in ROS 2 based on the work outlined in the paper “CoBe -- Coded
                Beacons for Localization,
                Object Tracking, and SLAM Augmentation” (Rabinovich et., 2017). In short infrared beacons, which all
                look like
                identical white blobs to a camera, can each be uniquely identified by flashing a binary ID number at a
                known baud rate,
                where bright is a 1 and dim is a 0. These newly identified beacons can then be passed to OpenCV’s
                Perspective-n-Point solver which in turn gives us our multi-rotor’s relative pose.</p>

            <p class="spaced">In addition to my localization package, I also setup a simple behavior tree test demo that
                would integrate the
                BehaviorTree.CPP library with ROS 2 (which was not a feature at the time), and I designed a simple
                mechatronics printed circuit board for the ground vehicle in Altium.</p>

            <figure>
                <video controls style="max-width: 100%; border-radius: 8px;" class="media">
                    <source src="media/gatech/behavior_tree_demo.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <figcaption>Simple behavior tree tech demo. In this video, we used a behavior tree to dequeue waypoints
                    from a list
                    and send commands to PX4 which would fly the quadcopter to said waypoints. After the list was empty,
                    the behavior tree
                    would automatically return the drone to home and land. Video credit Ethan Tse and AREAL.
                </figcaption>
            </figure>



        </details>

        <!-- Siemens -->
        <details>
            <summary>Siemens</summary>
            <p><strong>Title:</strong> Engineering Leadership Development Program (ELDP) Intern</p>
            <p><strong>Dates:</strong> May 2022 – August 2022</p>
            <h3>R&D Electronics - Solid State Circuit Breaker</h3>
            <p class="spaced">I spent my summer at Siemens assisting on various tasks on their new Solid State Circuit
                Breaker (SSCB).
                Compared to traditional circuit breakers which use an electromechanic switch, a SSCB uses MOSFETs to
                trip significantly
                faster than their electromechanical counterparts during a fault. In addition, these breakers utilize an
                STM32 microcontroller
                which allows for more advanced fault detection techniques.</p>

            <figure>
                <img src="media/siemens/Tester.jpg" alt="Ethan Testing the Device" class="media media-small">
                <figcaption>
                    I can't show any of the hardware since it's propreitary, so here's a picture of me crammed in behind
                    our big circuit breaker tester.
                    Photo credit Ethan Tse.
                </figcaption>
            </figure>

            <h4>Embedded Firmware</h4>
            <p>My primary responsibility was writing embedded C code for the SSCB. One of my tasks was to have the
                circuit breaker store current and
                voltage measurements over a fixed period of time, and then send that information via Bluetooth Low
                Energy (BLE) to a
                phone app. Another one of my tasks was to implement a “linear thermal memory” feature on the breaker.
                Essentially,
                the breaker would store its last trip time in an RTC (Real Time Clock), and on reboot, would modify its
                subsequent
                trip time based on how recently it had previously tripped.</p>
            <h4>LabVIEW Tester</h4>
            <p>My secondary task for the summer was to automate our I2t (current squared over time) tester, which
                essentially measures the
                trip time of a breaker for a given amount of current. Using LabVIEW, I wrote a program that would
                automatically command our
                tester via serial commands, record the trip time of the breaker, then re-set the breaker using a linear
                actuator.
                I then designed a graphical user interface for these tests.</p>
        </details>


    </main>

    <footer>
        &copy; 2025 Ethan Tse. All rights reserved.
    </footer>
</body>

</html>